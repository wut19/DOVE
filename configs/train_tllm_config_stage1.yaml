# general
# data_dir: data
gpu_config: configs/gpu_config_7b.json
cuda: 0
seed: 0
exps_path: exps_custom # exps_custom
train: True
val: True
test: False

# data preprocessing
flip_p: 0.

# encoder
use_vqvae: False
use_clip: 'openai/clip-vit-large-patch14'
freeze_encoder: True
encoder_path: {
                'color': '/root/autodl-tmp/exps_custom/2025_07_06_15_58_27_train_clip_color_0706/encoder.pt',
                'temperature': '/root/autodl-tmp/exps_custom/2025_07_06_16_08_30_train_clip_temperature_0706/encoder.pt',
                'texture': '/root/autodl-tmp/exps_custom/2025_07_06_16_13_48_train_clip_texture_0706/encoder.pt',
                'teng': '/root/autodl-tmp/exps_custom/2025_07_06_16_18_50_train_clip_teng_0706/encoder.pt'
}
encoder_output_size: 1024
num_context_vision: 8
prompt_depth_vision: 12
dim_context_vision: 1024
num_context_text: 6
prompt_depth_text: 12
dim_context_text: 768

# projection
freeze_projection: False
projection_lr: 0.0002
projection_path: null
# # you should specify the path to trained projection layer in second stage, e.g.
# {
#     'color': 'exps_custom/2024_08_20_14_21_25_train_tllm_train_val_vicuna-7b_30000/project_color_30000.pt',
#     'temperature': 'exps_custom/2024_08_20_14_21_25_train_tllm_train_val_vicuna-7b_30000/project_temperature_30000.pt',
#     'texture': 'exps_custom/2024_08_20_14_21_25_train_tllm_train_val_vicuna-7b_30000/project_texture_30000.pt',
#     'teng': 'exps_custom/2024_08_20_14_21_25_train_tllm_train_val_vicuna-7b_30000/project_teng_30000.pt'
# } 

# LLM
train_files: ['data/train_qa.json']
val_files: ['data/test_qa.json']
test_files: ['', '']
model_type: vicuna-7b
cutoff_len: 512
offload_dir: ./
llm_lr: 0.0002
quantized: False
tokenizer_path: null  # you should specify the path in second stage, e.g. exps_custom/2024_08_20_14_21_25_train_tllm_train_val_vicuna-7b_30000/tokenizer
llm_path: null # you should specify the path in second stage, e.g. exps_custom/2024_08_20_14_21_25_train_tllm_train_val_vicuna-7b_30000/llm_weights
## LoRA
lora_trained: False
use_lora: False # In the first stage: False; In the sencond stage, True
lora_alpha: 256
r: 128
lora_dropout: 0.05
target_modules:
  - q_proj
  - k_proj
modules_to_save:
  - embed_tokens
bias: none
## train
max_train_steps: 30000 # 30000 for the first stage, 5000 for second stage
save_freq: null
per_device_train_batch_size: 1
llm_gradient_accumulation_steps: 16
warmup_steps: 0.03
## val
per_device_val_batch_size: 1
## generation
max_new_tokens:
  train_object_property_description: 100
  train_object_comparison: 200
  train_object_reasoning: 500
  eval_object_property_description: 100
  eval_object_comparison: 200
  eval_object_reasoning: 500